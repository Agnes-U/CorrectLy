{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"I am looking forward to have a dinner with Saurav today.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I - nsubj\n",
      "am - aux\n",
      "looking - ROOT\n",
      "forward - advmod\n",
      "to - aux\n",
      "have - advcl\n",
      "a - det\n",
      "dinner - dobj\n",
      "with - prep\n",
      "Saurav - pobj\n",
      "today - npadvmod\n",
      ". - punct\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text + ' - ' + token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I nsubj looking VERB []\n",
      "am aux looking VERB []\n",
      "looking ROOT looking VERB [I, am, forward, have, .]\n",
      "forward advmod looking VERB []\n",
      "to aux have VERB []\n",
      "have advcl looking VERB [to, dinner, today]\n",
      "a det dinner NOUN []\n",
      "dinner dobj have VERB [a, with]\n",
      "with prep dinner NOUN [Saurav]\n",
      "Saurav pobj with ADP []\n",
      "today npadvmod have VERB []\n",
      ". punct looking VERB []\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.dep_, token.head.text, token.head.pos_,\n",
    "          [child for child in token.children])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data...\n",
      "Total addition questions: 50000\n",
      "Vectorization...\n",
      "Training Data:\n",
      "(45000, 7, 12)\n",
      "(45000, 4, 12)\n",
      "Validation Data:\n",
      "(5000, 7, 12)\n",
      "(5000, 4, 12)\n",
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 128)               72192     \n",
      "_________________________________________________________________\n",
      "repeat_vector_2 (RepeatVecto (None, 4, 128)            0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 4, 128)            131584    \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 4, 12)             1548      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 4, 12)             0         \n",
      "=================================================================\n",
      "Total params: 205,324\n",
      "Trainable params: 205,324\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 1\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 12s 272us/step - loss: 1.8892 - acc: 0.3212 - val_loss: 1.7918 - val_acc: 0.3436\n",
      "Q 580+417 T 997  \u001b[91m☒\u001b[0m 108 \n",
      "Q 22+96   T 118  \u001b[91m☒\u001b[0m 12  \n",
      "Q 216+525 T 741  \u001b[91m☒\u001b[0m 108 \n",
      "Q 53+940  T 993  \u001b[91m☒\u001b[0m 108 \n",
      "Q 48+534  T 582  \u001b[91m☒\u001b[0m 108 \n",
      "Q 803+26  T 829  \u001b[91m☒\u001b[0m 108 \n",
      "Q 678+429 T 1107 \u001b[91m☒\u001b[0m 108 \n",
      "Q 165+919 T 1084 \u001b[91m☒\u001b[0m 108 \n",
      "Q 449+74  T 523  \u001b[91m☒\u001b[0m 108 \n",
      "Q 45+831  T 876  \u001b[91m☒\u001b[0m 101 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 2\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 12s 257us/step - loss: 1.7354 - acc: 0.3591 - val_loss: 1.6542 - val_acc: 0.3810\n",
      "Q 967+77  T 1044 \u001b[91m☒\u001b[0m 106 \n",
      "Q 539+511 T 1050 \u001b[91m☒\u001b[0m 102 \n",
      "Q 5+103   T 108  \u001b[91m☒\u001b[0m 166 \n",
      "Q 626+857 T 1483 \u001b[91m☒\u001b[0m 1667\n",
      "Q 71+677  T 748  \u001b[91m☒\u001b[0m 777 \n",
      "Q 760+4   T 764  \u001b[91m☒\u001b[0m 666 \n",
      "Q 796+0   T 796  \u001b[91m☒\u001b[0m 166 \n",
      "Q 69+31   T 100  \u001b[91m☒\u001b[0m 16  \n",
      "Q 3+641   T 644  \u001b[91m☒\u001b[0m 666 \n",
      "Q 22+190  T 212  \u001b[91m☒\u001b[0m 226 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 3\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 13s 285us/step - loss: 1.5784 - acc: 0.4080 - val_loss: 1.4991 - val_acc: 0.4330\n",
      "Q 747+12  T 759  \u001b[91m☒\u001b[0m 774 \n",
      "Q 25+7    T 32   \u001b[91m☒\u001b[0m 10  \n",
      "Q 225+198 T 423  \u001b[91m☒\u001b[0m 594 \n",
      "Q 961+755 T 1716 \u001b[91m☒\u001b[0m 1774\n",
      "Q 907+78  T 985  \u001b[91m☒\u001b[0m 904 \n",
      "Q 960+694 T 1654 \u001b[91m☒\u001b[0m 1674\n",
      "Q 74+13   T 87   \u001b[91m☒\u001b[0m 13  \n",
      "Q 438+159 T 597  \u001b[91m☒\u001b[0m 544 \n",
      "Q 435+479 T 914  \u001b[91m☒\u001b[0m 104 \n",
      "Q 391+530 T 921  \u001b[91m☒\u001b[0m 104 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 4\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 12s 258us/step - loss: 1.4182 - acc: 0.4706 - val_loss: 1.3671 - val_acc: 0.4777\n",
      "Q 623+300 T 923  \u001b[91m☒\u001b[0m 866 \n",
      "Q 3+982   T 985  \u001b[91m☒\u001b[0m 986 \n",
      "Q 758+77  T 835  \u001b[91m☒\u001b[0m 836 \n",
      "Q 589+42  T 631  \u001b[91m☒\u001b[0m 508 \n",
      "Q 413+33  T 446  \u001b[92m☑\u001b[0m 446 \n",
      "Q 950+823 T 1773 \u001b[91m☒\u001b[0m 1639\n",
      "Q 992+737 T 1729 \u001b[91m☒\u001b[0m 1657\n",
      "Q 216+525 T 741  \u001b[91m☒\u001b[0m 666 \n",
      "Q 39+934  T 973  \u001b[91m☒\u001b[0m 906 \n",
      "Q 54+90   T 144  \u001b[91m☒\u001b[0m 147 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 5\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 11s 248us/step - loss: 1.2696 - acc: 0.5290 - val_loss: 1.2182 - val_acc: 0.5504\n",
      "Q 4+92    T 96   \u001b[91m☒\u001b[0m 91  \n",
      "Q 859+68  T 927  \u001b[91m☒\u001b[0m 956 \n",
      "Q 39+863  T 902  \u001b[91m☒\u001b[0m 966 \n",
      "Q 190+92  T 282  \u001b[91m☒\u001b[0m 299 \n",
      "Q 2+467   T 469  \u001b[91m☒\u001b[0m 461 \n",
      "Q 414+808 T 1222 \u001b[91m☒\u001b[0m 1221\n",
      "Q 56+812  T 868  \u001b[91m☒\u001b[0m 866 \n",
      "Q 885+994 T 1879 \u001b[91m☒\u001b[0m 1855\n",
      "Q 926+496 T 1422 \u001b[91m☒\u001b[0m 1431\n",
      "Q 842+12  T 854  \u001b[91m☒\u001b[0m 856 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 6\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 11s 252us/step - loss: 1.1541 - acc: 0.5741 - val_loss: 1.1087 - val_acc: 0.5886\n",
      "Q 56+126  T 182  \u001b[91m☒\u001b[0m 187 \n",
      "Q 930+52  T 982  \u001b[91m☒\u001b[0m 977 \n",
      "Q 717+7   T 724  \u001b[91m☒\u001b[0m 729 \n",
      "Q 945+7   T 952  \u001b[91m☒\u001b[0m 959 \n",
      "Q 470+71  T 541  \u001b[92m☑\u001b[0m 541 \n",
      "Q 20+633  T 653  \u001b[91m☒\u001b[0m 646 \n",
      "Q 580+34  T 614  \u001b[91m☒\u001b[0m 612 \n",
      "Q 84+408  T 492  \u001b[91m☒\u001b[0m 482 \n",
      "Q 66+674  T 740  \u001b[91m☒\u001b[0m 731 \n",
      "Q 7+468   T 475  \u001b[91m☒\u001b[0m 471 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 7\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 11s 247us/step - loss: 1.0548 - acc: 0.6129 - val_loss: 1.0406 - val_acc: 0.6099\n",
      "Q 952+71  T 1023 \u001b[91m☒\u001b[0m 1020\n",
      "Q 55+446  T 501  \u001b[91m☒\u001b[0m 500 \n",
      "Q 44+662  T 706  \u001b[91m☒\u001b[0m 710 \n",
      "Q 127+19  T 146  \u001b[91m☒\u001b[0m 140 \n",
      "Q 949+79  T 1028 \u001b[91m☒\u001b[0m 1020\n",
      "Q 33+62   T 95   \u001b[91m☒\u001b[0m 10  \n",
      "Q 78+377  T 455  \u001b[91m☒\u001b[0m 454 \n",
      "Q 728+308 T 1036 \u001b[91m☒\u001b[0m 100 \n",
      "Q 24+628  T 652  \u001b[91m☒\u001b[0m 650 \n",
      "Q 70+922  T 992  \u001b[91m☒\u001b[0m 980 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 8\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 11s 249us/step - loss: 0.9736 - acc: 0.6475 - val_loss: 0.9481 - val_acc: 0.6593\n",
      "Q 352+759 T 1111 \u001b[91m☒\u001b[0m 1110\n",
      "Q 20+94   T 114  \u001b[91m☒\u001b[0m 112 \n",
      "Q 378+48  T 426  \u001b[91m☒\u001b[0m 429 \n",
      "Q 368+834 T 1202 \u001b[91m☒\u001b[0m 1209\n",
      "Q 419+263 T 682  \u001b[91m☒\u001b[0m 696 \n",
      "Q 358+257 T 615  \u001b[91m☒\u001b[0m 619 \n",
      "Q 8+277   T 285  \u001b[91m☒\u001b[0m 289 \n",
      "Q 415+5   T 420  \u001b[91m☒\u001b[0m 419 \n",
      "Q 496+300 T 796  \u001b[91m☒\u001b[0m 799 \n",
      "Q 221+9   T 230  \u001b[91m☒\u001b[0m 221 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 9\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 11s 247us/step - loss: 0.9007 - acc: 0.6767 - val_loss: 0.8922 - val_acc: 0.6713\n",
      "Q 57+268  T 325  \u001b[91m☒\u001b[0m 322 \n",
      "Q 733+632 T 1365 \u001b[91m☒\u001b[0m 1355\n",
      "Q 945+7   T 952  \u001b[92m☑\u001b[0m 952 \n",
      "Q 142+566 T 708  \u001b[91m☒\u001b[0m 610 \n",
      "Q 505+383 T 888  \u001b[91m☒\u001b[0m 887 \n",
      "Q 9+186   T 195  \u001b[91m☒\u001b[0m 199 \n",
      "Q 34+605  T 639  \u001b[91m☒\u001b[0m 631 \n",
      "Q 535+3   T 538  \u001b[92m☑\u001b[0m 538 \n",
      "Q 758+707 T 1465 \u001b[91m☒\u001b[0m 1477\n",
      "Q 308+65  T 373  \u001b[91m☒\u001b[0m 374 \n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "'''An implementation of sequence to sequence learning for performing addition\n",
    "\n",
    "Input: \"535+61\"\n",
    "Output: \"596\"\n",
    "Padding is handled by using a repeated sentinel character (space)\n",
    "\n",
    "Input may optionally be reversed, shown to increase performance in many tasks in:\n",
    "\"Learning to Execute\"\n",
    "http://arxiv.org/abs/1410.4615\n",
    "and\n",
    "\"Sequence to Sequence Learning with Neural Networks\"\n",
    "http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf\n",
    "Theoretically it introduces shorter term dependencies between source and target.\n",
    "\n",
    "Two digits reversed:\n",
    "+ One layer LSTM (128 HN), 5k training examples = 99% train/test accuracy in 55 epochs\n",
    "\n",
    "Three digits reversed:\n",
    "+ One layer LSTM (128 HN), 50k training examples = 99% train/test accuracy in 100 epochs\n",
    "\n",
    "Four digits reversed:\n",
    "+ One layer LSTM (128 HN), 400k training examples = 99% train/test accuracy in 20 epochs\n",
    "\n",
    "Five digits reversed:\n",
    "+ One layer LSTM (128 HN), 550k training examples = 99% train/test accuracy in 30 epochs\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "from six.moves import range\n",
    "\n",
    "\n",
    "class CharacterTable(object):\n",
    "    \"\"\"Given a set of characters:\n",
    "    + Encode them to a one hot integer representation\n",
    "    + Decode the one hot integer representation to their character output\n",
    "    + Decode a vector of probabilities to their character output\n",
    "    \"\"\"\n",
    "    def __init__(self, chars):\n",
    "        \"\"\"Initialize character table.\n",
    "\n",
    "        # Arguments\n",
    "            chars: Characters that can appear in the input.\n",
    "        \"\"\"\n",
    "        self.chars = sorted(set(chars))\n",
    "        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n",
    "        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n",
    "\n",
    "    def encode(self, C, num_rows):\n",
    "        \"\"\"One hot encode given string C.\n",
    "\n",
    "        # Arguments\n",
    "            num_rows: Number of rows in the returned one hot encoding. This is\n",
    "                used to keep the # of rows for each data the same.\n",
    "        \"\"\"\n",
    "        x = np.zeros((num_rows, len(self.chars)))\n",
    "        for i, c in enumerate(C):\n",
    "            x[i, self.char_indices[c]] = 1\n",
    "        return x\n",
    "\n",
    "    def decode(self, x, calc_argmax=True):\n",
    "        if calc_argmax:\n",
    "            x = x.argmax(axis=-1)\n",
    "        return ''.join(self.indices_char[x] for x in x)\n",
    "\n",
    "\n",
    "class colors:\n",
    "    ok = '\\033[92m'\n",
    "    fail = '\\033[91m'\n",
    "    close = '\\033[0m'\n",
    "\n",
    "# Parameters for the model and dataset.\n",
    "TRAINING_SIZE = 50000\n",
    "DIGITS = 3\n",
    "REVERSE = True\n",
    "\n",
    "# Maximum length of input is 'int + int' (e.g., '345+678'). Maximum length of\n",
    "# int is DIGITS.\n",
    "MAXLEN = DIGITS + 1 + DIGITS\n",
    "\n",
    "# All the numbers, plus sign and space for padding.\n",
    "chars = '0123456789+ '\n",
    "ctable = CharacterTable(chars)\n",
    "\n",
    "questions = []\n",
    "expected = []\n",
    "seen = set()\n",
    "print('Generating data...')\n",
    "while len(questions) < TRAINING_SIZE:\n",
    "    f = lambda: int(''.join(np.random.choice(list('0123456789'))\n",
    "                    for i in range(np.random.randint(1, DIGITS + 1))))\n",
    "    a, b = f(), f()\n",
    "    # Skip any addition questions we've already seen\n",
    "    # Also skip any such that x+Y == Y+x (hence the sorting).\n",
    "    key = tuple(sorted((a, b)))\n",
    "    if key in seen:\n",
    "        continue\n",
    "    seen.add(key)\n",
    "    # Pad the data with spaces such that it is always MAXLEN.\n",
    "    q = '{}+{}'.format(a, b)\n",
    "    query = q + ' ' * (MAXLEN - len(q))\n",
    "    ans = str(a + b)\n",
    "    # Answers can be of maximum size DIGITS + 1.\n",
    "    ans += ' ' * (DIGITS + 1 - len(ans))\n",
    "    if REVERSE:\n",
    "        # Reverse the query, e.g., '12+345  ' becomes '  543+21'. (Note the\n",
    "        # space used for padding.)\n",
    "        query = query[::-1]\n",
    "    questions.append(query)\n",
    "    expected.append(ans)\n",
    "print('Total addition questions:', len(questions))\n",
    "\n",
    "print('Vectorization...')\n",
    "x = np.zeros((len(questions), MAXLEN, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(questions), DIGITS + 1, len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(questions):\n",
    "    x[i] = ctable.encode(sentence, MAXLEN)\n",
    "for i, sentence in enumerate(expected):\n",
    "    y[i] = ctable.encode(sentence, DIGITS + 1)\n",
    "\n",
    "# Shuffle (x, y) in unison as the later parts of x will almost all be larger\n",
    "# digits.\n",
    "indices = np.arange(len(y))\n",
    "np.random.shuffle(indices)\n",
    "x = x[indices]\n",
    "y = y[indices]\n",
    "\n",
    "# Explicitly set apart 10% for validation data that we never train over.\n",
    "split_at = len(x) - len(x) // 10\n",
    "(x_train, x_val) = x[:split_at], x[split_at:]\n",
    "(y_train, y_val) = y[:split_at], y[split_at:]\n",
    "\n",
    "print('Training Data:')\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print('Validation Data:')\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)\n",
    "\n",
    "# Try replacing GRU, or SimpleRNN.\n",
    "RNN = layers.LSTM\n",
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 128\n",
    "LAYERS = 1\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "# \"Encode\" the input sequence using an RNN, producing an output of HIDDEN_SIZE.\n",
    "# Note: In a situation where your input sequences have a variable length,\n",
    "# use input_shape=(None, num_feature).\n",
    "model.add(RNN(HIDDEN_SIZE, input_shape=(MAXLEN, len(chars))))\n",
    "# As the decoder RNN's input, repeatedly provide with the last hidden state of\n",
    "# RNN for each time step. Repeat 'DIGITS + 1' times as that's the maximum\n",
    "# length of output, e.g., when DIGITS=3, max output is 999+999=1998.\n",
    "model.add(layers.RepeatVector(DIGITS + 1))\n",
    "# The decoder RNN could be multiple layers stacked or a single layer.\n",
    "for _ in range(LAYERS):\n",
    "    # By setting return_sequences to True, return not only the last output but\n",
    "    # all the outputs so far in the form of (num_samples, timesteps,\n",
    "    # output_dim). This is necessary as TimeDistributed in the below expects\n",
    "    # the first dimension to be the timesteps.\n",
    "    model.add(RNN(HIDDEN_SIZE, return_sequences=True))\n",
    "\n",
    "# Apply a dense layer to the every temporal slice of an input. For each of step\n",
    "# of the output sequence, decide which character should be chosen.\n",
    "model.add(layers.TimeDistributed(layers.Dense(len(chars))))\n",
    "model.add(layers.Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Train the model each generation and show predictions against the validation\n",
    "# dataset.\n",
    "for iteration in range(1, 10):\n",
    "    print()\n",
    "    print('-' * 50)\n",
    "    print('Iteration', iteration)\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=BATCH_SIZE,\n",
    "              epochs=1,\n",
    "              validation_data=(x_val, y_val))\n",
    "    # Select 10 samples from the validation set at random so we can visualize\n",
    "    # errors.\n",
    "    for i in range(10):\n",
    "        ind = np.random.randint(0, len(x_val))\n",
    "        rowx, rowy = x_val[np.array([ind])], y_val[np.array([ind])]\n",
    "        preds = model.predict_classes(rowx, verbose=0)\n",
    "        q = ctable.decode(rowx[0])\n",
    "        correct = ctable.decode(rowy[0])\n",
    "        guess = ctable.decode(preds[0], calc_argmax=False)\n",
    "        print('Q', q[::-1] if REVERSE else q, end=' ')\n",
    "        print('T', correct, end=' ')\n",
    "        if correct == guess:\n",
    "            print(colors.ok + '☑' + colors.close, end=' ')\n",
    "        else:\n",
    "            print(colors.fail + '☒' + colors.close, end=' ')\n",
    "        print(guess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 2, 14, 15, 1, 16, 17, 18, 1, 3, 19, 20, 21]\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "\n",
    "data = \"\"\" Jack and Jill went up the hill\\n\n",
    "\t\tTo fetch a pail of water\\n\n",
    "\t\tJack fell down and broke his crown\\n\n",
    "\t\tAnd Jill came tumbling after\\n \"\"\"\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([data])\n",
    "encoded = tokenizer.texts_to_sequences([data])[0]\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 1]\n",
      "[1, 3]\n",
      "[3, 4]\n",
      "[4, 5]\n",
      "[5, 6]\n",
      "[6, 7]\n",
      "[7, 8]\n",
      "[8, 9]\n",
      "[9, 10]\n",
      "[10, 11]\n",
      "[11, 12]\n",
      "[12, 13]\n",
      "[13, 2]\n",
      "[2, 14]\n",
      "[14, 15]\n",
      "[15, 1]\n",
      "[1, 16]\n",
      "[16, 17]\n",
      "[17, 18]\n",
      "[18, 1]\n",
      "[1, 3]\n",
      "[3, 19]\n",
      "[19, 20]\n",
      "[20, 21]\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "sequences = list()\n",
    "for i in range(1, len(encoded)):\n",
    "    sequence = encoded[i - 1 : i + 1]\n",
    "    print(sequence)\n",
    "    sequences.append(sequence)\n",
    "\n",
    "print(len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = array(sequences)\n",
    "x, y = sequences[:, 0], sequences[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 10, input_length=1))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dense(vocab_size, activation='softmax'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
