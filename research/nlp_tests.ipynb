{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('We', 'PRP'),\n",
       " ('are', 'VBP'),\n",
       " ('going', 'VBG'),\n",
       " ('to', 'TO'),\n",
       " ('the', 'DT'),\n",
       " ('party', 'NN')]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "text = nltk.word_tokenize(\"We are going to the party\")\n",
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.parse.generate import generate, demo_grammar\n",
    "from nltk import CFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  S -> NP VP\n",
      "  NP -> Det N\n",
      "  PP -> P NP\n",
      "  VP -> 'slept' | 'saw' NP | 'walked' PP\n",
      "  Det -> 'the' | 'a'\n",
      "  N -> 'man' | 'park' | 'dog'\n",
      "  P -> 'in' | 'with'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(demo_grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating the first 23 sentences for demo grammar:\n",
      "\n",
      "  S -> NP VP\n",
      "  NP -> Det N\n",
      "  PP -> P NP\n",
      "  VP -> 'slept' | 'saw' NP | 'walked' PP\n",
      "  Det -> 'the' | 'a'\n",
      "  N -> 'man' | 'park' | 'dog'\n",
      "  P -> 'in' | 'with'\n",
      "\n",
      "  1. the man slept\n",
      "  2. the man saw the man\n",
      "  3. the man saw the park\n",
      "  4. the man saw the dog\n",
      "  5. the man saw a man\n",
      "  6. the man saw a park\n",
      "  7. the man saw a dog\n",
      "  8. the man walked in the man\n",
      "  9. the man walked in the park\n",
      " 10. the man walked in the dog\n",
      " 11. the man walked in a man\n",
      " 12. the man walked in a park\n",
      " 13. the man walked in a dog\n",
      " 14. the man walked with the man\n",
      " 15. the man walked with the park\n",
      " 16. the man walked with the dog\n",
      " 17. the man walked with a man\n",
      " 18. the man walked with a park\n",
      " 19. the man walked with a dog\n",
      " 20. the park slept\n",
      " 21. the park saw the man\n",
      " 22. the park saw the park\n",
      " 23. the park saw the dog\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import itertools\n",
    "import sys\n",
    "from nltk.grammar import Nonterminal\n",
    "\n",
    "\n",
    "def generate(grammar, start=None, depth=None, n=None):\n",
    "    \"\"\"\n",
    "    Generates an iterator of all sentences from a CFG.\n",
    "\n",
    "    :param grammar: The Grammar used to generate sentences.\n",
    "    :param start: The Nonterminal from which to start generate sentences.\n",
    "    :param depth: The maximal depth of the generated tree.\n",
    "    :param n: The maximum number of sentences to return.\n",
    "    :return: An iterator of lists of terminal tokens.\n",
    "    \"\"\"\n",
    "    if not start:\n",
    "        start = grammar.start()\n",
    "    if depth is None:\n",
    "        depth = sys.maxsize\n",
    "\n",
    "    iter = _generate_all(grammar, [start], depth)\n",
    "\n",
    "    if n:\n",
    "        iter = itertools.islice(iter, n)\n",
    "\n",
    "    return iter\n",
    "\n",
    "\n",
    "\n",
    "def _generate_all(grammar, items, depth):\n",
    "    if items:\n",
    "        try:\n",
    "            for frag1 in _generate_one(grammar, items[0], depth):\n",
    "                for frag2 in _generate_all(grammar, items[1:], depth):\n",
    "                    yield frag1 + frag2\n",
    "        except RuntimeError as _error:\n",
    "            if _error.message == \"maximum recursion depth exceeded\":\n",
    "                # Helpful error message while still showing the recursion stack.\n",
    "                raise RuntimeError(\"The grammar has rule(s) that yield infinite recursion!!\")\n",
    "            else:\n",
    "                raise\n",
    "    else:\n",
    "        yield []\n",
    "\n",
    "\n",
    "def _generate_one(grammar, item, depth):\n",
    "    if depth > 0:\n",
    "        if isinstance(item, Nonterminal):\n",
    "            for prod in grammar.productions(lhs=item):\n",
    "                for frag in _generate_all(grammar, prod.rhs(), depth-1):\n",
    "                    yield frag\n",
    "        else:\n",
    "            yield [item]\n",
    "\n",
    "demo_grammar = \"\"\"\n",
    "  S -> NP VP\n",
    "  NP -> Det N\n",
    "  PP -> P NP\n",
    "  VP -> 'slept' | 'saw' NP | 'walked' PP\n",
    "  Det -> 'the' | 'a'\n",
    "  N -> 'man' | 'park' | 'dog'\n",
    "  P -> 'in' | 'with'\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def demo(N=23):\n",
    "    from nltk.grammar import CFG\n",
    "\n",
    "    print('Generating the first %d sentences for demo grammar:' % (N,))\n",
    "    print(demo_grammar)\n",
    "    grammar = CFG.fromstring(demo_grammar)\n",
    "    for n, sent in enumerate(generate(grammar, n=N), 1):\n",
    "        print('%3d. %s' % (n, ' '.join(sent)))\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "prepchoices = nltk.ConditionalFreqDist((v[0], p[0]) \n",
    "    for (v, p) in nltk.bigrams(brown.tagged_words(tagset=\"universal\")) \n",
    "        if v[1] == \"VERB\" and p[1] == \"ADP\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'in': 5, 'at': 3, 'from': 3, 'to': 2, 'on': 1, 'for': 1, 'about': 1, 'since': 1, 'under': 1, 'with': 1})"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepchoices[\"writing\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = {}\n",
    "grammar[\"sitting\"] = {}\n",
    "grammar[\"sitting\"][\"table\"] = \"on\"\n",
    "grammar[\"sitting\"][\"van\"] = \"in\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sitting': {'van': 'in', 'table': 'on'}}\n"
     ]
    }
   ],
   "source": [
    "print(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saurav]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "sent = \"when the bell rang, saurav went out\"\n",
    "doc=nlp(sent)\n",
    "\n",
    "sub_toks = [tok for tok in doc if (tok.dep_ == \"nsubj\") ]\n",
    "\n",
    "print(sub_toks) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello, world.', 'Here are two sentences.']\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function\n",
    "\n",
    "raw_text = 'Hello, world. Here are two sentences.'\n",
    "nlp = spacy.load('en')\n",
    "doc = nlp(raw_text)\n",
    "sentences = [sent.string.strip() for sent in doc.sents]\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 7425985699627899538 the\n",
      "shop 15809682053778148938 shop\n",
      "is 10382539506755952630 be\n",
      "closed 16417442958758597567 close\n",
      ". 12646065887601541794 .\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "doc = nlp(u\"the shop is closed.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token, token.lemma, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noun_chunks(doc, drop_determiners=True, min_freq=1):\n",
    "    \"\"\"\n",
    "    Extract an ordered sequence of noun chunks from a spacy-parsed doc, optionally\n",
    "    filtering by frequency and dropping leading determiners.\n",
    "    Args:\n",
    "        doc (``textacy.Doc`` or ``spacy.Doc``)\n",
    "        drop_determiners (bool): remove leading determiners (e.g. \"the\")\n",
    "            from phrases (e.g. \"the quick brown fox\" => \"quick brown fox\")\n",
    "        min_freq (int): remove chunks that occur in ``doc`` fewer than\n",
    "            ``min_freq`` times\n",
    "    Yields:\n",
    "        ``spacy.Span``: the next noun chunk from ``doc`` in order of appearance\n",
    "        in the document\n",
    "    \"\"\"\n",
    "    if hasattr(doc, 'spacy_doc'):\n",
    "        ncs = doc.spacy_doc.noun_chunks\n",
    "    else:\n",
    "        ncs = doc.noun_chunks\n",
    "    if drop_determiners is True:\n",
    "        ncs = (nc if nc[0].pos != DET else nc[1:]\n",
    "               for nc in ncs)\n",
    "    if min_freq > 1:\n",
    "        ncs = list(ncs)\n",
    "        freqs = itertoolz.frequencies(nc.lower_ for nc in ncs)\n",
    "        ncs = (nc for nc in ncs\n",
    "               if freqs[nc.lower_] >= min_freq)\n",
    "\n",
    "    for nc in ncs:\n",
    "        yield nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object noun_chunks at 0x7f1477330a98>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noun_chunks(\"the boy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        jumps                    \n",
      "  ________|______________         \n",
      " |        |             over     \n",
      " |        |              |        \n",
      " |       fox            dog      \n",
      " |    ____|_____      ___|____    \n",
      " .  The quick brown the      lazy\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "from nltk import Tree\n",
    "\n",
    "\n",
    "en_nlp = spacy.load('en')\n",
    "\n",
    "doc = en_nlp(\"The quick brown fox jumps over the lazy dog.\")\n",
    "\n",
    "def to_nltk_tree(node):\n",
    "    if node.n_lefts + node.n_rights > 0:\n",
    "        return Tree(node.orth_, [to_nltk_tree(child) for child in node.children])\n",
    "    else:\n",
    "        return node.orth_\n",
    "\n",
    "\n",
    "[to_nltk_tree(sent.root).pretty_print() for sent in doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      walking_VBG              \n",
      "   ________|______________      \n",
      "  |        |       |   down_IN \n",
      "  |        |       |      |     \n",
      "  |        |       |  street_NN\n",
      "  |        |       |      |     \n",
      "I_PRP    am_VBP   ._.   the_DT \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "from nltk import Tree\n",
    "\n",
    "\n",
    "en_nlp = spacy.load('en')\n",
    "\n",
    "doc = en_nlp(\"I am walking down the street.\")\n",
    "\n",
    "def tok_format(tok):\n",
    "    return \"_\".join([tok.orth_, tok.tag_])\n",
    "\n",
    "\n",
    "def to_nltk_tree(node):\n",
    "    if node.n_lefts + node.n_rights > 0:\n",
    "        return Tree(tok_format(node), [to_nltk_tree(child) for child in node.children])\n",
    "    else:\n",
    "        return tok_format(node)\n",
    "\n",
    "\n",
    "[to_nltk_tree(sent.root).pretty_print() for sent in doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       swimming_VBG         \n",
      "   _________|__________      \n",
      "  |         |        in_IN  \n",
      "  |         |          |     \n",
      "  |         |       river_NN\n",
      "  |         |          |     \n",
      "He_PRP   was_VBD     the_DT \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2 = en_nlp(\"He was swimming in the river\")\n",
    "[to_nltk_tree(sent.root).pretty_print() for sent in doc2.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               came_VBD                                                 \n",
      "  ________________________________|______________________________________________        \n",
      " |    |      |     |           got_VBD                    to_IN                  |      \n",
      " |    |      |     |      ________|________                 |                    |       \n",
      " |    |      |     |     |        |     email_NN         house_NN           started_VBD \n",
      " |    |      |     |     |        |        |         _______|_________           |       \n",
      ",_, he_PRP and_CC ._. When_WRB  he_PRP   the_DT  my_PRP$ small_JJ office_NN shouting_VBG\n",
      "\n",
      "came\n",
      "to\n",
      "house\n"
     ]
    }
   ],
   "source": [
    "doc3 = en_nlp(\"When he got the email, he came to my small office house and started shouting.\")\n",
    "[to_nltk_tree(sent.root).pretty_print() for sent in doc3.sents]\n",
    "for sent in doc3.sents:\n",
    "    print(sent.root)\n",
    "    for ch in sent.root.children:\n",
    "        if(ch.tag_ == \"IN\"):\n",
    "            print(ch)\n",
    "            for sec in ch.children:\n",
    "                print(sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      walking_VBG            \n",
      "   ________|_____________     \n",
      "  |        |       |   on_IN \n",
      "  |        |       |     |    \n",
      "  |        |       |  road_NN\n",
      "  |        |       |     |    \n",
      "I_PRP    am_VBP   ._.  the_DT\n",
      "\n",
      "walking\n",
      "on\n",
      "road\n"
     ]
    }
   ],
   "source": [
    "doc3 = en_nlp(\"I am walking on the road.\")\n",
    "[to_nltk_tree(sent.root).pretty_print() for sent in doc3.sents]\n",
    "for sent in doc3.sents:\n",
    "    print(sent.root)\n",
    "    for ch in sent.root.children:\n",
    "        if(ch.tag_ == \"IN\"):\n",
    "            print(ch)\n",
    "            for sec in ch.children:\n",
    "                print(sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         playing_VBG                             \n",
      "    __________|_____________________________      \n",
      "   |                    |                 in_IN  \n",
      "   |                    |                   |     \n",
      "   |                 boys_NNS           garden_NN\n",
      "   |           _________|_________          |     \n",
      "were_VBD    The_DT            little_JJ   the_DT \n",
      "\n",
      "playing\n",
      "in\n",
      "garden\n"
     ]
    }
   ],
   "source": [
    "doc3 = en_nlp(\"The little boys were playing in the garden\")\n",
    "[to_nltk_tree(sent.root).pretty_print() for sent in doc3.sents]\n",
    "for sent in doc3.sents:\n",
    "    print(sent.root)\n",
    "    for ch in sent.root.children:\n",
    "        if(ch.tag_ == \"IN\"):\n",
    "            print(ch)\n",
    "            for sec in ch.children:\n",
    "                print(sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       found_VBN                                          \n",
      "  _________________________|__________________________________________     \n",
      " |      |         |        |      |                 |               in_IN \n",
      " |      |         |        |      |                 |                 |    \n",
      " |      |         |        |      |            confusion_NN        case_NN\n",
      " |      |         |        |      |       __________|_________        |    \n",
      ",_, Salman_NNP was_VBD guilty_JJ ._. Admist_VB              all_DT  the_DT\n",
      "\n",
      "found\n",
      "in\n",
      "case\n"
     ]
    }
   ],
   "source": [
    "doc3 = en_nlp(\"Admist all confusion, Salman was found guilty in the case.\")\n",
    "[to_nltk_tree(sent.root).pretty_print() for sent in doc3.sents]\n",
    "for sent in doc3.sents:\n",
    "    print(sent.root)\n",
    "    for ch in sent.root.children:\n",
    "        if(ch.tag_ == \"IN\"):\n",
    "            print(ch)\n",
    "            for sec in ch.children:\n",
    "                print(sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   cooking_VBG                                                           \n",
      "    ____________________________________|_______________________________________                          \n",
      "   |        |       |        |                     |                       playing_VBG                   \n",
      "   |        |       |        |                     |                  __________|___________________      \n",
      "   |        |       |        |                   in_IN               |          |         |       in_IN  \n",
      "   |        |       |        |                     |                 |          |         |         |     \n",
      "   |        |       |    mother_NN             kitchen_NN            |          |      boys_NNS garden_NN\n",
      "   |        |       |        |           __________|_________        |          |         |         |     \n",
      "was_VBD dinner_NN and_CC   The_DT     the_DT              home_NN were_VBD     ._.      the_DT    the_DT \n",
      "\n",
      "cooking\n",
      "in\n",
      "kitchen\n"
     ]
    }
   ],
   "source": [
    "doc3 = en_nlp(\"The mother was cooking dinner in the home kitchen and the boys were playing in the garden.\")\n",
    "[to_nltk_tree(sent.root).pretty_print() for sent in doc3.sents]\n",
    "for sent in doc3.sents:\n",
    "    print(sent.root)\n",
    "    for ch in sent.root.children:\n",
    "        if(ch.tag_ == \"IN\"):\n",
    "            print(ch)\n",
    "            for sec in ch.children:\n",
    "                print(sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
